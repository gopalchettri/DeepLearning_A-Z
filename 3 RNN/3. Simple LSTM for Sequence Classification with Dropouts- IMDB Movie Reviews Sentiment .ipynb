{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words (5000 in this case), zero the rest\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only considering maximum review lenth to 500 words\n",
    "max_review_length = 500\n",
    "\n",
    "# Truncate and pad the input sequences so that they are all the same length for modeling i.e., 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 55s 126ms/step - loss: 0.4671 - accuracy: 0.7773 - val_loss: 0.3211 - val_accuracy: 0.8692\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 49s 126ms/step - loss: 0.3304 - accuracy: 0.8656 - val_loss: 0.3542 - val_accuracy: 0.8501\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.2896 - accuracy: 0.8824 - val_loss: 0.3673 - val_accuracy: 0.8396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b0199cd90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating LSTM model\n",
    "\n",
    "# The first layer is the Embedded layer that uses 32 length vectors to represent each word\n",
    "# The next layer is the LSTM layer with 100 memory units (smart neurons).\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=top_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(units=100)) # rest properties have the default property values\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 21s 27ms/step - loss: 0.3673 - accuracy: 0.8396\n",
      "Accuracy: 83.96%\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "## LSTM for Sequence Classification with Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 52s 128ms/step - loss: 0.4859 - accuracy: 0.7683 - val_loss: 0.3609 - val_accuracy: 0.8455\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 51s 132ms/step - loss: 0.3105 - accuracy: 0.8762 - val_loss: 0.3069 - val_accuracy: 0.8726\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.3500 - accuracy: 0.8522 - val_loss: 0.3291 - val_accuracy: 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b042dec70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating model with dropouts\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(input_dim=top_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "model_1.add(Dropout(rate=0.2))\n",
    "model_1.add(LSTM(units=100))\n",
    "model_1.add(Dropout(rate=0.2))\n",
    "model_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_1.summary())\n",
    "\n",
    "model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.56%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "scores = model_1.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Sequence Classification with Recurrent  Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "213/391 [===============>..............] - ETA: 13:16 - loss: 0.5569 - accuracy: 0.6987"
     ]
    }
   ],
   "source": [
    "# creating model with dropouts\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(input_dim=top_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "model_2.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_2.summary())\n",
    "\n",
    "model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "scores = model_2.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-26",
   "language": "python",
   "name": "tf-gpu-26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
