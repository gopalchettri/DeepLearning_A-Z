{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but keep the top n words (5000 in this case), zero the rest\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only considering maximum review length to 500 words\n",
    "max_review_length = 500\n",
    "\n",
    "# Truncate and pad the input sequences so that they are all the same length for modeling i.e., 500\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 39s 75ms/step - loss: 0.4249 - accuracy: 0.7916 - val_loss: 0.2848 - val_accuracy: 0.8829\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2438 - accuracy: 0.9033 - val_loss: 0.3291 - val_accuracy: 0.8642\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2016 - accuracy: 0.9234 - val_loss: 0.3033 - val_accuracy: 0.8807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23692a48160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating LSTM mode with cnn\n",
    "\n",
    "# The first layer is the Embedded layer that uses 32 length vectors to represent each word\n",
    "# The next layer is the LSTM layer with 100 memory units (smart neurons).\n",
    "\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=top_words, output_dim=embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(LSTM(units=100)) # rest properties have the default property value\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 17ms/step - loss: 0.3033 - accuracy: 0.8807\n",
      "Accuracy : 88.07%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy : %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-26",
   "language": "python",
   "name": "tf-gpu-26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
